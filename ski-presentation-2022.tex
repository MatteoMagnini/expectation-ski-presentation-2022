%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS Beamer series / Bologna FC / Template
% Andrea Omicini
% Alma Mater Studiorum - Universit√† di Bologna
% mailto:andrea.omicini@unibo.it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{default}}
%
\documentclass[presentation]{beamer}\mode<presentation>{\usetheme{AMSBolognaFC}}
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{AMSBolognaFC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{ski-presentation-2022}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[SKI: Symbolic Knowledge Injection]
{SKI: Symbolic Knowledge Injection}
%
\subtitle[state of the art and our current works]
{state of the art and our current works}
%
\author[\sspeaker{Magnini}]
{\speaker{Matteo Magnini}\\\href{mailto:matteo.magnini@unibo.it}{matteo.magnini@unibo.it}}
%
\institute[DISI, Univ.\ Bologna]
{Dipartimento di Informatica -- Scienza e Ingegneria (DISI)\\\textsc{Alma Mater Studiorum} -- Universit{\`a} di Bologna}
%
\date[\today]{\today}
%

\AtBeginSection[]
{
    %\\\\\\\\\\\\\\\\\\\\\
    \begin{frame}<beamer>[c,noframenumbering]
        \frametitle{Next in Line\ldots}
        \tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide]
    \end{frame}
    %\\\\\\\\\\\\\\\\\\\\\
}
\AtBeginSubsection[]
{
    %\\\\\\\\\\\\\\\\\\\\\
    \begin{frame}<beamer>[shrink,noframenumbering]
        \frametitle{Focus on\ldots}
        \mbox{~}
        \tableofcontents[currentsubsection,sectionstyle=shaded,subsectionstyle=show/shaded]
        \mbox{~}
    \end{frame}
    %\\\\\\\\\\\\\\\\\\\\\
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%/////////
\frame{\titlepage}
%/////////

%%===============================================================================
%\section*{Outline}
%%===============================================================================
%
%%/////////
%\frame[c]{\tableofcontents[hideallsubsections]}
%%/////////

%===============================================================================
\section{Premises}
%===============================================================================

%/////////
\begin{frame}[c]{Definition}
%
We define symbolic knowledge injection as:
%
\begin{displayquote}\itshape
    any \emph{algorithmic} procedure affecting how \textcolor{bolognafcred}{sub-symbolic predictors} draw their inferences in such a way that predictions are either \emph{computed} as a function of, or made \emph{consistent} with, some \emph{given} \textcolor{bolognafcred}{symbolic knowledge}.
\end{displayquote}
%
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Symbolic Knowledge}
    %
    A symbolic representation consists of:
    %
    \begin{enumerate}
        \item a set of symbols;
        \item\label{item:symbolic-combination} a set of grammatical rules governing the combining of symbols; 
        \item\label{item:symbolic-assignment} elementary symbols and any admissible combination of them can be assigned with meaning.
        %
        \begin{itemize}
            \item[$\Rightarrow$] Symbolic knowledge is both human and machine interpretable,
            \item first order logic (FOL) is an example of symbolic representation.
        \end{itemize}
    \end{enumerate}
    
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Sub-symbolic data}
    \begin{itemize}
        \item ML methods, and sub-symbolic approaches in general, represent data as arrays of real numbers, and knowledge as functions over such data;
        %
        \item despite numbers are technically symbols as well, we cannot consider arrays and their functions as symbolic knowledge representation (KR) means;
        %
        \item sub-symbolic approaches frequently violate \Cref{item:symbolic-combination,item:symbolic-assignment}.
        %
    \end{itemize}
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Sub-symbolic predictors}
    %
    \begin{itemize}
        \item deep neural networks (DNN);
        \begin{itemize}
            \item convolutional neural networks (CNN),
            \item recurrent neural networks (RNN);
        \end{itemize}
        \item kernel machines;
        \item others.
    \end{itemize}
    %
    \vfill
    %
    The vast majority of predictors are NN most probably because they are easy to manipulate and they have top performances.
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Why SKI?}
    %
    There are several benefits:
    %
\begin{itemize}
    \item reduce learning time;
    %
    \item reduce the data size needed for training;
    %
    \item improve predictor's accuracy;
    %
    \item build a predictor that behave as a logic engine.
\end{itemize}
%
\end{frame}
%/////////


%===============================================================================
\section{Taxonomy}
%===============================================================================

%/////////
\begin{frame}[c]{Aim}
    %
    \begin{block}{Enrich (learning support)}
        \begin{itemize}
            \item reduce learning time;
            %
            \item reduce the data size needed for training;
            %
            \item improve predictor's accuracy.
        \end{itemize}
    \end{block}
    %
    \begin{block}{Manifold (symbolic knowledge manipulation)}
        \begin{itemize}
            \item logic inference;
            %
            \item information retrieval;
            %
            \item knowledge base completion/fusion.
        \end{itemize}
    \end{block}
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Predictors}
    %
    \begin{itemize}
        \item theoretically, one can inject prior knowledge into any sub-symbolic predictor;
        %
        \item in practice, NN are almost the sole predictors treated in literature;
        %
        \item however, lot of different NN architecture are considered.
    \end{itemize}
    %
    \begin{figure}
        \centering
        \includegraphics[width=.6\linewidth]{figures/neuron.png}
    \end{figure}
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{How}
    %
    There exist three major ways to perform knowledge injection on sub-symbolic predictors:
    %
    \begin{itemize}
        \item constraining, a cost factor proportional to the violation of the knowledge is introduced during learning;
        \item structuring, the architecture of the predictor is built in such a way to mimic the knowledge;
        \item embedding, the symbolic knowledge is embedded into a tensor form and it is given in input as training data to the predictor.
   \end{itemize} 
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Constraining}
    %
    \begin{itemize}
        \item Knowledge cost factor is introduced in the loss function;
        %
        \item for NN the cost affects backpropagation during training.
        %
        \begin{itemize}
            \item[$\Rightarrow$] Predictor does not violate the prior knowledge (to a certain extent)
        \end{itemize} 
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/ski-constraining}
    \end{figure}
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Structuring}
    %
    \begin{itemize}
        \item Inner architecture is shaped to be able to ``mimic'' the knowledge;
        %
        \item for NN this means \emph{ad-hoc} layers.
        %
        \begin{itemize}
            \item[$\Rightarrow$] Predictor directly exploits knowledge when needed.
        \end{itemize} 
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/ski-structuring}
    \end{figure}
    %
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Embedding}
    %
    \begin{itemize}
        \item Symbolic knowledge is embedded into a tensor form;
        %
        \item this is used as predictor's input data (alone or with a ``standard'' dataset).
        %
        \begin{itemize}
            \item[$\Rightarrow$] Predictor's aim is manifold in most cases.
        \end{itemize} 
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/ski-embedding}
    \end{figure}
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic}
    \begin{block}{Intensional}
        \begin{itemize}
            \item indirect representation of data,
            %
            \item define a relation/set by describing its elements via other relations/sets.
        \end{itemize}
        %
    \end{block}
    %
    \begin{block}{Extensional}
        \begin{itemize}
            \item direct representation of data,
            %
            \item explicit definition of entities involved.
        \end{itemize}
    \end{block}
    %
    \vfill
    %
    Recursive intensional predicates are very expressive and powerful, as they enable the description of infinite sets via a finite (and commonly small) amount of formul\ae.
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic}
    %
    Almost the totality of SKI algorithms deal with:
    %
    \begin{itemize}
        \item first order logic (FOL);
        %
        \item knowledge graph (KG);
        %
        \item propositional logic (PL).
        
    \end{itemize}
    %
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic: FOL}
    \begin{itemize}
        \item FOL is extremely flexible and expressive;
        \item you can use recursion and define recursive structures;
        \item maybe too ``powerful'' for canonic NN.
        \begin{itemize}
            \item[$\Rightarrow$] Most NN are natively DAG (directed acyclic graph)
            \item this allow backpropagation as training algorithm but ...
            \item how can you support recursion?  
        \end{itemize}
    \end{itemize}
    \centering\vfill
    %
    \phantom{You can't!}
    %
    \phantom{Unless you use some tricks.}
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic: FOL}
    \begin{itemize}
        \item FOL is extremely flexible and expressive;
        \item you can use recursion and define recursive structures;
        \item maybe too ``powerful'' for canonic NN.
        \begin{itemize}
            \item[$\Rightarrow$] Most NN are natively DAG (directed acyclic graph)
            \item this allow backpropagation as training algorithm but ...
            \item how can you support recursion?  
        \end{itemize}
    \end{itemize}
    \centering\vfill
    %
    You can't!
    %
    Unless you use some tricks.
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic: KG}
    %
    \begin{itemize}
        \item Only constants, variables and n-ary predicates with $n < 3$;
        \item collections of triplets $\langle \functor{a}\ \predication{f}\ \functor{b} \rangle$ or $\predication{f}(\functor{a}, \functor{b})$
        \item essentially directed graph:
        \begin{itemize}
            \item nodes $\Rightarrow$ individuals,
            \item vertices $\Rightarrow$ properties connecting individuals;
        \end{itemize}
        \item may instantiate an ontology, i.e., a formal description of classes characterising a given domain.
    \end{itemize}
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic: KG}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/kg-example}
    \end{figure}    
\end{frame}
%/////////

%/////////
\begin{frame}[c]{Logic: PL}
    %
    \begin{itemize}
        \item No quantifiers, terms, and non-atomic predicates;
        \item expressions involving one or many 0-ary predicates (propositions) possibly interconnected by ordinary logic connectives;
        \item low expressiveness, but easy to work with.
    \end{itemize}
    %
    \vfill\centering
    $p \wedge \neg q \rightarrow r$
\end{frame}
%/////////


%===============================================================================
\section{Literature overview}
%===============================================================================

%/////////
\begin{frame}[c]{First works}
    %
\end{frame}
%/////////


%/////////
\begin{frame}[c]{Notable works}
    %
\end{frame}
%/////////

%===============================================================================
\section{\psyki}
%===============================================================================


%/////////
\begin{frame}{General SKI workflow}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textheight]{figures/ski-workflow.pdf}
    \end{figure}
    
\end{frame}
%/////////

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}{1 -- Parsing}
    
    \begin{minipage}{0.5\textwidth}
        \begin{equation*}
            \begin{split}
                class&(X_{-30}, \dots, X_{30}, ie) \leftarrow\\
                &\textit{pyramidine-rich}(\dots)\ \wedge\\
                &X_{-3} = y\ \wedge\\
                &X_{-2} = a\ \wedge\\
                &X_{-1} = g\ \wedge\\
                &X_{1} = g\ \wedge\\
                &\neg(\textit{ie-stop}(\dots)) 
            \end{split}
        \end{equation*}
    \end{minipage}
    \noindent\begin{minipage}{0.4\textwidth}% adapt widths of minipages to your needs
        \includegraphics[width=\linewidth]{figures/ast-ie-1.pdf}
    \end{minipage}%
    
    
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}{2 -- Fuzzification}
    
    \begin{minipage}{0.6\textwidth}
        \input{tables/fuzzification}
        
        \begin{center}\scriptsize
            $^{*}$ encodes the value for the $i^{th}$ output
            \\
            \smallskip
            $^{**}$ assuming $p$ is defined by $k$ clauses of the form:
            \\
            $\pred{p}(\bar{X}) \leftarrow \psi_1,\ \ldots,\ \pred{p}(\bar{X}) \leftarrow \psi_k$
        \end{center}
    \end{minipage}
    %
    \begin{minipage}{0.35\textwidth}
        \begin{equation*}
            \begin{split}
                class&(X_{-30}, \dots, X_{30}, ie) \leftarrow\\
                &X_{-3} = y\ \wedge\\
                &X_{-2} = a\ \wedge\\
                &X_{-1} = g\ \wedge\\
                &X_{1} = g
            \end{split}
        \end{equation*}
        %
        \centering
        $\downarrow$
        \begin{equation*}
            \begin{split}
                min\{&min\{min\{1-|X_{-3} - y|,\\
                &\phantom{min\{min\{}1-|X_{-2} - a|\},\\
                &\phantom{min\{}1-|X_{-1} - g|\},\\
                &1-|X_{1}-g|\}
            \end{split}
        \end{equation*}
    \end{minipage}
    
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%/////////
\begin{frame}[c]{3 -- Injection}
    %
    Injection step is algorithm specific but it falls back into the three following approaches already discussed:
    \begin{block}{Injection families}
        \begin{itemize}
            \item constraining;
            \item structuring;
            \item embedding.
        \end{itemize}
    \end{block}
\end{frame}
%/////////

%/////////
\begin{frame}[c]{4 -- Training}
    %
\end{frame}
%/////////

%===============================================================================
\section{Open literature research lines}
%===============================================================================


%/////////
\begin{frame}[c]{SKE \& SKI}
    %
\end{frame}
%/////////


%/////////
\begin{frame}[c]{Other scientific fields}
    %
\end{frame}
%/////////

%===============================================================================
\section*{}
%===============================================================================

%/////////
\frame{\titlepage}
%/////////

%===============================================================================
\section*{\refname}
%===============================================================================

%%%%
\setbeamertemplate{page number in head/foot}{}
%/////////
\begin{frame}[c,noframenumbering]{\refname}
%\begin{frame}[t,allowframebreaks,noframenumbering]{\refname}
%	\tiny
	\scriptsize
%	\footnotesize
	\bibliographystyle{apalike-AMS}
	\bibliography{AMSBolognaFC-template}
\end{frame}
%/////////

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
